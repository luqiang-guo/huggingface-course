## 训练

pytorch 版本的API更像是在torch的基础上，做了一些扩展。

- ### 数据加载
    将数据预处理之后缓存到文件系统。
    ```
    from transformers import AutoTokenizer
    from datasets import load_dataset
    # 加载数据集
    raw_datasets = load_dataset("glue", "mrpc")

    checkpoint = "bert-base-uncased"
    # 实例化预处理
    tokenizer = AutoTokenizer.from_pretrained(checkpoint)
    inputs = tokenizer("This is the first sentence.", "This is the second one.")
    # {'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996, 2117, 2028, 1012, 102], 
    # 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], 
    # 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}

    def tokenize_function(example):
        return tokenizer(example["sentence1"], example["sentence2"], truncation=True)

    # 将数据集缓存到文件系统，这么操作相对于直接加载到内存可以节省很多空间。
    tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
    ```

- ### 动态填充

    通过动态批次填充的方式减少内存的使用。
    ```
    # 选取一部分数据做演示
    samples = tokenized_datasets["train"][:8]
    samples = {
        k: v for k, v in samples.items() if k not in ["idx", "sentence1", "sentence2"]
    }
    # print([len(x) for x in samples["input_ids"]])

    # 这批次数据最长是67
    # [50, 59, 47, 67, 59, 50, 62, 32]


    from transformers import DataCollatorWithPadding

    # 将数据填充到此批次的最大值，而不是输入的最大值，(节省内存)
    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

    batch = data_collator(samples)
    # print({k: v.shape for k, v in batch.items()})
    # {'attention_mask': torch.Size([8, 67]),
    #  'input_ids': torch.Size([8, 67]),
    #  'token_type_ids': torch.Size([8, 67]),
    #  'labels': torch.Size([8])}

    ```

- ### 高阶API训练

    使用更高层次的API Trainer 进行训练，相当于在pytorch基础上封装了一层。
    ```
    from datasets import load_dataset
    from transformers import AutoTokenizer, DataCollatorWithPadding

    raw_datasets = load_dataset("glue", "mrpc")
    checkpoint = "bert-base-uncased"

    # 将原始数据转成训练所需要的数据
    tokenizer = AutoTokenizer.from_pretrained(checkpoint)

    def tokenize_function(example):
        return tokenizer(example["sentence1"], example["sentence2"], truncation=True)
    # 预处理数据集，并缓存到磁盘
    tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)

    # 动态填充数据
    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

    from transformers import TrainingArguments

    # 获取参数
    training_args = TrainingArguments("test-trainer")
    print(training_args)

    from transformers import AutoModelForSequenceClassification

    # 实例化模型
    model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)

    from transformers import Trainer


    trainer = Trainer(
        model,
        training_args,
        train_dataset=tokenized_datasets["train"],
        eval_dataset=tokenized_datasets["validation"],
        data_collator=data_collator,
        tokenizer=tokenizer,
    )
    #训练
    trainer.train()

    # 监测
    predictions = trainer.predict(tokenized_datasets["validation"])
    print(predictions.predictions.shape, predictions.label_ids.shape)
    ```

- ### Pytorch 风格训练
    采用pytorch层次的api进行训练。可以完全随心所欲的修改训练模型。

  - Dataloader加载数据
    ```
    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

    # 删除与参数不相符的list
    tokenized_datasets = tokenized_datasets.remove_columns(
        ["sentence1", "sentence2", "idx"]
    )

    from torch.utils.data import DataLoader

    # dataloader 加载数据
    train_dataloader = DataLoader(
        tokenized_datasets["train"], shuffle=True, batch_size=8, collate_fn=data_collator
    )
    eval_dataloader = DataLoader(
        tokenized_datasets["validation"], batch_size=8, collate_fn=data_collator
    )
    ```

  - 实例化lr_scheduler

    ```
    num_epochs = 3
    num_training_steps = num_epochs * len(train_dataloader)

    # lr_scheduler
    lr_scheduler = get_scheduler(
        "linear",
        optimizer=optimizer,
        num_warmup_steps=0,
        num_training_steps=num_training_steps
    )
    ```
  - 训练与验证

    ```
    model.train()

    for batch in train_dataloader:
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(**batch)
        loss = outputs.loss
        loss.backward()
        
        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)


    from datasets import load_metric

    metric= load_metric("glue", "mrpc")
    model.eval()
    for batch in eval_dataloader:
        batch = {k: v.to(device) for k, v in batch.items()}
        with torch.no_grad():
            outputs = model(**batch)
        
        logits = outputs.logits
        predictions = torch.argmax(logits, dim=-1)
        metric.add_batch(predictions=predictions, references=batch["labels"])
        
    metric.compute()
    ```

  - 完整训练代码
    
    ```
    from datasets import load_dataset
    from transformers import AutoTokenizer, DataCollatorWithPadding
    from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler
    from tqdm.auto import tqdm
    import torch
    from torch.utils.data import DataLoader
    from datasets import load_metric

    # 加载数据
    raw_datasets = load_dataset("glue", "mrpc")
    checkpoint = "bert-base-uncased"
    # 预处理数据
    tokenizer = AutoTokenizer.from_pretrained(checkpoint)

    def tokenize_function(example):
        return tokenizer(example["sentence1"], example["sentence2"], truncation=True)

    # 预处理到文件
    tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)

    # 数据加载
    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

    # 删除不需要的list  (参数是需要)。
    tokenized_datasets = tokenized_datasets.remove_columns(
        ["sentence1", "sentence2", "idx"]
    )

    # pytorch  dataloader
    train_dataloader = DataLoader(
        tokenized_datasets["train"], shuffle=True, batch_size=8, collate_fn=data_collator
    )
    eval_dataloader = DataLoader(
        tokenized_datasets["validation"], batch_size=8, collate_fn=data_collator
    )

    # 实例化 model
    model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
    # 实例化 optimizer
    optimizer = AdamW(model.parameters(), lr=3e-5)
    # 设置device
    device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
    model.to(device)

    num_epochs = 3
    num_training_steps = num_epochs * len(train_dataloader)

    # lr_scheduler
    lr_scheduler = get_scheduler(
        "linear",
        optimizer=optimizer,
        num_warmup_steps=0,
        num_training_steps=num_training_steps
    )

    # 进度条
    progress_bar = tqdm(range(num_training_steps))

    # 训练
    model.train()
    for epoch in range(num_epochs):
        for batch in train_dataloader:
            batch = {k: v.to(device) for k, v in batch.items()}
            outputs = model(**batch)
            loss = outputs.loss
            loss.backward()
            
            optimizer.step()
            lr_scheduler.step()
            optimizer.zero_grad()
            progress_bar.update(1)

    metric= load_metric("glue", "mrpc")
    model.eval()
    for batch in eval_dataloader:
        batch = {k: v.to(device) for k, v in batch.items()}
        with torch.no_grad():
            outputs = model(**batch)
        
        logits = outputs.logits
        predictions = torch.argmax(logits, dim=-1)
        metric.add_batch(predictions=predictions, references=batch["labels"])
        
    metric.compute()
    ```
## 分布式训练

  Huggingface 提供了Accelerate 的接口，用来实现分布式操作。支持多机多卡数据并行。
  Accelerate提供了pytorch和deepspeed两种分布式策略。

- ### 设置Accelerate参数
    通过命令行设置运行参数
    ```
    In which compute environment are you running? ([0] This machine, [1] AWS (Amazon SageMaker)): 0
    Which type of machine are you using? ([0] No distributed training, [1] multi-CPU, [2] multi-GPU, [3] TPU): 2
    How many different machines will you use (use more than 1 for multi-node training)? [1]: 1
    Do you want to use DeepSpeed? [yes/NO]: no
    How many processes in total will you use? [1]: 2        # 2 卡
    Do you wish to use FP16 (mixed precision)? [yes/NO]: no
    ```

    通过代码设置运行参数
    ```
    ...
    deepspeed_plugin = DeepSpeedPlugin(zero_stage=2, gradient_accumulation_steps=2)
    accelerator = Accelerator(fp16=True, deepspeed_plugin=deepspeed_plugin)
    ...
    ```
 
- ### 用Accelerate修改源码来支持分布式运行

  - Dataloader

    ```
     - # 数据加载
     - collate_fn = DataCollatorWithPadding(tokenizer=tokenizer)
     + def collate_fn(examples):
     +      # On TPU it's best to pad everything to the same length or training will be very slow.
     +      if accelerator.distributed_type == DistributedType.TPU:
     +          return tokenizer.pad(examples, padding="max_length", max_length=128, return_tensors="pt")
     +      return tokenizer.pad(examples, padding="longest", return_tensors="pt")

    # Instantiate dataloaders.
    train_dataloader = DataLoader(
        tokenized_datasets["train"], shuffle=True, collate_fn=collate_fn, batch_size=batch_size
    )
    eval_dataloader = DataLoader(
        tokenized_datasets["validation"], shuffle=False, collate_fn=collate_fn, batch_size=batch_size
    )
    ```

  - Device设置
    ```
     - model.to(device)
     + model.to(accelerator.device)
    ```

  - 预处理
    ```
    model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
        model, optimizer, train_dataloader, eval_dataloader
    )
    ```

  - Train 

    ```
    ...
    model.train()
    for epoch in range(10):
        for source, targets in data:
    -         source = source.to(device)
    -         targets = targets.to(device)

            optimizer.zero_grad()

            output = model(source)
            loss = F.cross_entropy(output, targets)

    -         loss.backward()
    +         accelerator.backward(loss)

            optimizer.step()
    ...
    ```

- ### Demo
  - 单机2卡
    -  accelerate config 配置环境
    ```
        In which compute environment are you running? ([0] This machine, [1] AWS (Amazon SageMaker)): 0
        Which type of machine are you using? ([0] No distributed training, [1] multi-CPU, [2] multi-GPU, [3] TPU): 2
        How many different machines will you use (use more than 1 for multi-node training)? [1]: 1
        Do you want to use DeepSpeed? [yes/NO]: no
        How many processes in total will you use? [1]: 2
        Do you wish to use FP16 (mixed precision)? [yes/NO]: no
    ```
    - 运行`accelerate launch distribution2.py`，就可以2卡运行
    ```
      ...
        INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:
        entrypoint       : ./distribution2.py
        min_nodes        : 1
        max_nodes        : 1
        nproc_per_node   : 2
        run_id           : none
        rdzv_backend     : static
        rdzv_endpoint    : 127.0.0.1:29500
        rdzv_configs     : {'rank': 0, 'timeout': 900}
        max_restarts     : 3
        monitor_interval : 5
        log_dir          : None
        metrics_cfg      : {}
        ...
    ```
    - distribution2.py
    ```
    from datasets import load_dataset
    from tqdm.auto import tqdm
    import torch
    from torch.utils.data import DataLoader
    from datasets import load_metric
    from accelerate import Accelerator
    from transformers import (
        AdamW,
        AutoModelForSequenceClassification,
        AutoTokenizer,
        get_linear_schedule_with_warmup,
        set_seed,
        get_scheduler,
        DataCollatorWithPadding,
    )


    MAX_GPU_BATCH_SIZE = 16
    EVAL_BATCH_SIZE = 32

    #
    accelerator = Accelerator()

    # 加载原始数据
    raw_datasets = load_dataset("glue", "mrpc")
    checkpoint = "bert-base-cased"
    # 预处理数据
    tokenizer = AutoTokenizer.from_pretrained(checkpoint)

    def tokenize_function(examples):
            # max_length=None => use the model max length (it's actually the default)
            outputs = tokenizer(examples["sentence1"], examples["sentence2"], truncation=True, max_length=None)
            return outputs

    # 预处理到文件
    tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)

    # 
    tokenized_datasets = raw_datasets.map(
        tokenize_function,
        batched=True,
        remove_columns=["idx", "sentence1", "sentence2"],    # 删除不需要的列
    )

    # 修改为labels 
    # 模型的
    tokenized_datasets.rename_column_("label", "labels")

    # If the batch size is too big we use gradient accumulation
    gradient_accumulation_steps = 1
    batch_size = 8

    def collate_fn(examples):
        # On TPU it's best to pad everything to the same length or training will be very slow.
        if accelerator.distributed_type == DistributedType.TPU:
            return tokenizer.pad(examples, padding="max_length", max_length=128, return_tensors="pt")
        return tokenizer.pad(examples, padding="longest", return_tensors="pt")

    # Instantiate dataloaders.
    train_dataloader = DataLoader(
        tokenized_datasets["train"], shuffle=True, collate_fn=collate_fn, batch_size=batch_size
    )
    eval_dataloader = DataLoader(
        tokenized_datasets["validation"], shuffle=False, collate_fn=collate_fn, batch_size=batch_size
    )

    # 数据加载
    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)


    # pytorch  dataloader
    train_dataloader = DataLoader(
        tokenized_datasets["train"], shuffle=True, batch_size=8, collate_fn=data_collator
    )
    eval_dataloader = DataLoader(
        tokenized_datasets["validation"], batch_size=8, collate_fn=data_collator
    )

    seed = 34
    set_seed(seed)

    # Instantiate the model (we build the model here so that the seed also control new weights initialization)
    model = AutoModelForSequenceClassification.from_pretrained(checkpoint, return_dict=True)

    # 设置device
    model.to(accelerator.device)

    # Instantiate optimizer
    optimizer = AdamW(params=model.parameters(), lr=2e-5, correct_bias=True)

    # Prepare everything
    # There is no specific order to remember, we just need to unpack the objects in the same order we gave them to the
    # prepare method.
    model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
        model, optimizer, train_dataloader, eval_dataloader
    )

    num_epochs = 3
    num_training_steps = num_epochs * len(train_dataloader)

    # Instantiate learning rate scheduler after preparing the training dataloader as the prepare method
    # may change its length.
    lr_scheduler = get_linear_schedule_with_warmup(
        optimizer=optimizer,
        num_warmup_steps=100,
        num_training_steps=len(train_dataloader) * num_epochs,
    )



    # 进度条
    progress_bar = tqdm(range(num_training_steps))

    # 训练
    # Now we train the model
    for epoch in range(num_epochs):
        model.train()
        for step, batch in enumerate(train_dataloader):
            # We could avoid this line since we set the accelerator with `device_placement=True`.
            batch.to(accelerator.device)
            outputs = model(**batch)
            loss = outputs.loss
            loss = loss / gradient_accumulation_steps
            accelerator.backward(loss)
            if step % gradient_accumulation_steps == 0:
                optimizer.step()
                lr_scheduler.step()
                optimizer.zero_grad()
            progress_bar.update(1)


    ```
