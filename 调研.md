## Hugging Face
https://github.com/huggingface/transformers
同时适配TensorFlow和Pytorch

## 快速使用

- ### 概述
  可以使用`pipeline`用几行代码快速验证结果，开箱即用不进行finetune直接完成任务。

    ```
    from transformers import pipeline

    classifier = pipeline("sentiment-analysis")
    classifier("I've been waiting for a HuggingFace course my whole life.")
    # 输出结果[{'label': 'POSITIVE', 'score': 0.9598048329353333}]

    classifier = pipeline("zero-shot-classification")
    classifier(
        "This is a tuntun",
        candidate_labels=["jingjing", "politics", "business"],
    )
    # 输出结果{'sequence': 'This is a tuntun', 'labels': ['jingjing', 'business', 'politics'], 'scores': [0.568869948387146, 0.24648480117321014, 0.18464526534080505]}

    generator = pipeline("text-generation")
    print(generator("In this course, we will teach you how to"))
    # 输出结果 [{'generated_text': 'In this course, we will teach you how to be a bit more professional. We will also show you how to build something that is truly amazing and unique with your efforts. We will not cover all topics. This is to allow you to make something'}]

    generator = pipeline("text-generation", model="distilgpt2")
    generator(
        "In this course, we will teach you how to",
        max_length=30,
        num_return_sequences=2,
    )
    # 输出结果 [{'generated_text': 'In this course, we will teach you how to work with the web server, using both APIs and libraries in a single application.\n\n\n\n'}, {'generated_text': 'In this course, we will teach you how to use a few common techniques to manipulate the internal state of your body. The following exercises will be required'}]

    unmasker = pipeline("fill-mask")
    unmasker("This course will teach you all about <mask> models.", top_k=2)
    # 输出结果 [{'sequence': 'This course will teach you all about mathematical models.', 'score': 0.19619855284690857, 'token': 30412, 'token_str': ' mathematical'}, {'sequence': 'This course will teach you all about computational models.', 'score': 0.04052741825580597, 'token': 38163, 'token_str': ' computational'}]

    ner = pipeline("ner", grouped_entities=True)
    ner("My name is Sylvain and I work at Hugging Face in Brooklyn.")
    # 输出结果 [{'entity_group': 'PER', 'score': 0.9981694, 'word': 'Sylvain', 'start': 11, 'end': 18}, {'entity_group': 'ORG', 'score': 0.9796019, 'word': 'Hugging Face', 'start': 33, 'end': 45}, {'entity_group': 'LOC', 'score': 0.99321055, 'word': 'Brooklyn', 'start': 49, 'end': 57}]

    question_answerer = pipeline("question-answering")
    question_answerer(
        question="Where do I work?",
        context="My name is Sylvain and I work at Hugging Face in Brooklyn"
    )
    # 输出结果 {'score': 0.6949757933616638, 'start': 33, 'end': 45, 'answer': 'Hugging Face'}

    summarizer = pipeline("summarization")
    summarizer("""
        America has changed dramatically during recent years. Not only has the number of 
        graduates in traditional engineering disciplines such as mechanical, civil, 
        electrical, chemical, and aeronautical engineering declined, but in most of 
        the premier American universities engineering curricula now concentrate on 
        and encourage largely the study of engineering science. As a result, there 
        are declining offerings in engineering subjects dealing with infrastructure, 
        the environment, and related issues, and greater concentration on high 
        technology subjects, largely supporting increasingly complex scientific 
        developments. While the latter is important, it should not be at the expense 
        of more traditional engineering.

        Rapidly developing economies such as China and India, as well as other 
        industrial countries in Europe and Asia, continue to encourage and advance 
        the teaching of engineering. Both China and India, respectively, graduate 
        six and eight times as many traditional engineers as does the United States. 
        Other industrial countries at minimum maintain their output, while America 
        suffers an increasingly serious decline in the number of engineering graduates 
        and a lack of well-educated engineers.

    # 输出结果 [{'summary_text': ' America has changed dramatically during recent years . The number of engineering graduates in the U.S. has declined in traditional engineering disciplines such as mechanical, civil,    electrical, chemical, and aeronautical engineering . Rapidly developing economies such as China and India continue to encourage and advance the teaching of engineering .'}]
    """)
    ```


## 进阶使用
  提供了更底层得api，可以和pytorch交互使用，更加灵活更加方便。

- 数据预处理
  将输出文本处理成模型所需要的输入。

  例如:
  ```
  from transformers import AutoTokenizer

  checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
  tokenizer = AutoTokenizer.from_pretrained(checkpoint)

  raw_inputs = [
      "I've been waiting for a HuggingFace course my whole life.", 
      "I hate this so much!",
  ]
  # 处理成pytorch类型的输出结果
  inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors="pt")
  # {'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,
          2607,  2026,  2878,  2166,  1012,   102],
        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,
             0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}
  ```

- 数据预处理API

    - 编码
    ```
    from transformers import AutoTokenizer
    tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")

    sequence = "Using a Transformer network is simple"

    # step 1
    tokens = tokenizer.tokenize(sequence)
    print(tokens)
    # ['Using', 'a', 'transform', '##er', 'network', 'is', 'simple']

    # step 2
    ids = tokenizer.convert_tokens_to_ids(tokens)
    print(ids)
    [7993, 170, 11303, 1200, 2443, 1110, 3014]
    ```

    - 解码
    ```
    decode_string = tokenizer.decode([ 7993 , 170 , 11303 , 1200 , 2443 , 1110 , 3014 ]) 
    print(decoded_string)
    'Using a Transformer network is simple'
    ```

  - 多序列数据填充
    
    以下内容不能直接转成tensor
    
    ```
    batched_ids = [
    [200, 200, 200],
    [200, 200]
    ]
    ```
    通过填充数据的方式最终转成需要的结果。
    ```
    batched_ids = [
        [200, 200, 200],
        [200, 200, tokenizer.pad_token_id]
    ]

    attention_mask = [
        [1, 1, 1],
        [1, 1, 0]
    ]

    outputs = model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask))
    print(outputs.logits)
    ```

    - 填充
    ```
    # 将序列填充到最大序列长度
    model_inputs = tokenizer(sequences, padding="longest")
    
    # 将序列填充到模型最大长度
    # (512 for BERT or DistilBERT)
    model_inputs = tokenizer(sequences, padding="max_length")
    
    # 对于小于max_length的填充
    model_inputs = tokenizer(sequences, padding="max_length", max_length=8)
    
    ```
        
    - 截断

    ```
    # 截断到模型最大输入长度
    # (512 for BERT or DistilBERT)
    model_inputs = tokenizer(sequences, truncation=True)

    # 截断超过指定长度
    model_inputs = tokenizer(sequences, max_length=8, truncation=True)
    # {'input_ids': [[101, 1045, 1005, 2310, 2042, 3403, 2005, 102], [101, 2061, 2031, 1045, 999, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}

    ```

    输出结果可以转换到相应框架

    ```
    sequences = [
        "I've been waiting for a HuggingFace course my whole life.",
        "So have I!"
    ]

    # Returns PyTorch tensors
    model_inputs = tokenizer(sequences, padding=True, return_tensors="pt")

    # Returns TensorFlow tensors
    model_inputs = tokenizer(sequences, padding=True, return_tensors="tf")

    # Returns NumPy arrays
    model_inputs = tokenizer(sequences, padding=True, return_tensors="np")
    ```
    
   总结
    ```
    import torch
    from transformers import AutoTokenizer, AutoModelForSequenceClassification

    checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
    tokenizer = AutoTokenizer.from_pretrained(checkpoint)
    model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
    sequences = [
    "I've been waiting for a HuggingFace course my whole life.",
    "So have I!"
    ]

    tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors="pt")
    output = model(**tokens)

    ```


- 模型的加载与保存

  - 加载随机初始化的模型
  ```
  from transformers import BertConfig, BertModel

  config = BertConfig()
  
  BertConfig {
    [...]
    "hidden_size": 768,
    "intermediate_size": 3072,
    "max_position_embeddings": 512,
    "num_attention_heads": 12,
    "num_hidden_layers": 12,
    [...]
  }

  model = BertModel(config)

  # Model is randomly initialized!
  ```

  - 加载训练好的base bert模型
  ```
  # 直接从远程仓库加载模型，或者从本地加载
  # https://huggingface.co/bert-base-cased  
  # 从官网下载的模型默认缓存到 `~/.cache/huggingface/transformers`

  model = BertModel.from_pretrained("bert-base-cased")
  ```
  - 保存模型
  ```
  # 保存模型到本地
  model.save_pretrained("directory_on_my_computer")
  ```

- 模型输出
```
  from transformers import AutoTokenizer
  from transformers import AutoModel

  checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
  tokenizer = AutoTokenizer.from_pretrained(checkpoint)

  raw_inputs = [
      "I've been waiting for a HuggingFace course my whole life.", 
      "I hate this so much!",
  ]
  # 处理成pytorch类型的输出结果
  inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors="pt")
  
  checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
  model = AutoModel.from_pretrained(checkpoint)
  outputs = model(**inputs)

  # 模型输出
  BaseModelOutput(last_hidden_state=tensor([[[-0.1798,  0.2333,  0.6321,  ..., -0.3017,  0.5008,  0.1481],
         [ 0.2758,  0.6497,  0.3200,  ..., -0.0760,  0.5136,  0.1329],
         [ 0.9046,  0.0985,  0.2950,  ...,  0.3352, -0.1407, -0.6464],
         ...,
         [ 0.1466,  0.5661,  0.3235,  ..., -0.3376,  0.5100, -0.0561],
         [ 0.7500,  0.0487,  0.1738,  ...,  0.4684,  0.0030, -0.6084],
         [ 0.0519,  0.3729,  0.5223,  ...,  0.3584,  0.6500, -0.3883]],

        [[-0.2937,  0.7283, -0.1497,  ..., -0.1187, -1.0227, -0.0422],
         [-0.2206,  0.9384, -0.0951,  ..., -0.3643, -0.6605,  0.2407],
         [-0.1536,  0.8987, -0.0728,  ..., -0.2189, -0.8528,  0.0710],
         ...,
         [-0.3017,  0.9002, -0.0200,  ..., -0.1082, -0.8412, -0.0861],
         [-0.3338,  0.9674, -0.0729,  ..., -0.1952, -0.8181, -0.0634],
         [-0.3454,  0.8824, -0.0426,  ..., -0.0993, -0.8329, -0.1065]]],
       grad_fn=<NativeLayerNormBackward>), hidden_states=None, attentions=None)

  # 模型的输出结果是一个三维的tensor outputs shape`torch.Size([ 2 , 16 , 768 ])`具体含义如下：
  # Batch size: The number of sequences processed at a time (2 in our example).
  # Sequence length: The length of the numerical representation of the sequence (16 in our example).
  # Hidden size: The vector dimension of each model input.

```